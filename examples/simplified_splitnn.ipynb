{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "epochs = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplified Split Neural Network (Chipirones)\n",
    "\n",
    "\n",
    "In this example, computation graphs have been reworked to be distributed in PySyft. This means we can do away with the cumbersome backwards method. The data has been reworked to be a simple dummy dataset. This makes debugging wayy easier as we add horizontal distributions.\n",
    "\n",
    "<b>Description: </b> Here we fold a multilayer SplitNN in on itself in order to accomodate the data nd labels being in the same place. We demonstrate the SplitNN class with a 3 segment distribution. This time,\n",
    "\n",
    "<img src=\"images/FoldedNN.png\" width=\"20%\">\n",
    "\n",
    "- <b>Alice</b>\n",
    "    - Has Model Segment 1\n",
    "    - Has Model Segment 3\n",
    "    - Has the handwritten images\n",
    "    - Has the image labels\n",
    "- <b>Bob</b>\n",
    "    - Has model Segment 2\n",
    "    \n",
    "Again, we use the exact same model as we used in the previous tutorial and see the same accuracy. Neither Alice nor Bob have the full model and Bob can't see Alice's data. \n",
    "\n",
    "Author:\n",
    "- Adam J Hall - Twitter: [@AJH4LL](https://twitter.com/AJH4LL) Â· GitHub:  [@H4LL](https://github.com/H4LL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitNN:\n",
    "    def __init__(self, models, optimizers):\n",
    "        self.models = models\n",
    "        self.optimizers = optimizers\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a = []\n",
    "        \n",
    "        a.append(models[0](x))\n",
    "        if a[-1].location == models[1].location:\n",
    "            a[-1] = a[-1].detach().requires_grad_()\n",
    "        else:\n",
    "            a[-1] = a[-1].detach().move(models[1].location).requires_grad_()\n",
    "\n",
    "        i=1    \n",
    "        while i < (len(models)-1):    \n",
    "            a.append(models[i](a[-1]))\n",
    "            if a[-1].location == models[i+1].location:\n",
    "                a[-1] = a[-1].detach().requires_grad_()\n",
    "            else:\n",
    "                a[-1] = a[-1].detach().move(models[i+1].location).requires_grad_() \n",
    "            i+=1\n",
    "        \n",
    "        a.append(models[i](a[-1]))\n",
    "        self.a = a\n",
    "        \n",
    "        return a[-1]\n",
    "\n",
    "    \n",
    "    def zero_grads(self):\n",
    "        for opt in optimizers:\n",
    "            opt.zero_grad()\n",
    "        \n",
    "    def step(self):\n",
    "        for opt in optimizers:\n",
    "            opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import syft as sy\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "# create some workers\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "claire = sy.VirtualWorker(hook, id=\"claire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Toy Dataset\n",
    "data = torch.tensor([[0,0,0,0],[0,1,0,0],[1,0,0,0],[1,1,0,0.]], requires_grad=True)\n",
    "labels = torch.tensor([[0],[0],[1],[1.]], requires_grad=True)\n",
    "\n",
    "data = data.send(alice)\n",
    "labels = labels.send(claire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Define our model segments\n",
    "\n",
    "input_size = 4\n",
    "hidden_sizes = [3, 2]\n",
    "output_size = 1\n",
    "\n",
    "models = [\n",
    "    nn.Linear(input_size, hidden_sizes[0]),\n",
    "    nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    nn.Linear(hidden_sizes[1], output_size)\n",
    "    ]\n",
    "\n",
    "# Create optimisers for each segment and link to them\n",
    "optimizers = [\n",
    "    optim.SGD(model.parameters(), lr=0.03,)\n",
    "    for model in models\n",
    "]\n",
    "\n",
    "# Send Model Segments to model locations\n",
    "model_locations = [alice, bob, claire]\n",
    "for model, location in zip(models, model_locations):\n",
    "    model.send(location)\n",
    "\n",
    "#Instantiate a SpliNN class with our distributed segments and their respective optimizers\n",
    "splitNN = SplitNN(models, optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, target, splitNN):\n",
    "    \n",
    "    #1) Zero our grads\n",
    "    splitNN.zero_grads()\n",
    "    \n",
    "    #2) Make a prediction\n",
    "    pred = splitNN.forward(x)\n",
    "    \n",
    "    print(pred.location)\n",
    "    \n",
    "    #3) Figure out how much we missed by\n",
    "    loss = ((pred - target)**2).sum()\n",
    "\n",
    "    #4) Backprop the loss on the end layer\n",
    "    loss.backward()\n",
    "    \n",
    "    #6) Change the weights\n",
    "    splitNN.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<VirtualWorker id:claire #objects:5>\n",
      "Epoch 0 - Training loss: 1.3407090902328491\n",
      "<VirtualWorker id:claire #objects:5>\n",
      "Epoch 1 - Training loss: 1.2212690114974976\n",
      "<VirtualWorker id:claire #objects:5>\n",
      "Epoch 2 - Training loss: 1.1569126844406128\n",
      "<VirtualWorker id:claire #objects:5>\n",
      "Epoch 3 - Training loss: 1.1220707893371582\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    running_loss = 0\n",
    "    loss = train(data, labels, splitNN)\n",
    "    running_loss += loss.get()\n",
    "\n",
    "#     else:\n",
    "    print(\"Epoch {} - Training loss: {}\".format(i, running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
