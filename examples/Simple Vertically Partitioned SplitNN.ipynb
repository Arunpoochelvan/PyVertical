{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Simple Vertically Partitioned Split Neural Network\n",
    "\n",
    "- <b>Alice</b>\n",
    "    - Has model Segment 1\n",
    "    - Has the handwritten Images\n",
    "- <b>Bob</b>\n",
    "    - Has model Segment 2\n",
    "    - Has the image Labels\n",
    "    \n",
    "Based on [SplitNN - Tutorial 3](https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/advanced/split_neural_network/Tutorial%203%20-%20Folded%20Split%20Neural%20Network.ipynb) from Adam J Hall - Twitter: [@AJH4LL](https://twitter.com/AJH4LL) · GitHub:  [@H4LL](https://github.com/H4LL)\n",
    "\n",
    "Authors:\n",
    "- Pavlos Papadopoulos · GitHub:  [@pavlos-p](https://github.com/pavlos-p)\n",
    "- Tom Titcombe · GitHub:  [@TTitcombe](https://github.com/TTitcombe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitNN:\n",
    "    def __init__(self, models, optimizers):\n",
    "        self.models = models\n",
    "        self.optimizers = optimizers\n",
    "\n",
    "        self.data = []\n",
    "        self.remote_tensors = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        data = []\n",
    "        remote_tensors = []\n",
    "\n",
    "        data.append(models[0](x))\n",
    "\n",
    "        if data[-1].location == models[1].location:\n",
    "            remote_tensors.append(data[-1].detach().requires_grad_())\n",
    "        else:\n",
    "            remote_tensors.append(\n",
    "                data[-1].detach().move(models[1].location).requires_grad_()\n",
    "            )\n",
    "\n",
    "        i = 1\n",
    "        while i < (len(models) - 1):\n",
    "            data.append(models[i](remote_tensors[-1]))\n",
    "\n",
    "            if data[-1].location == models[i + 1].location:\n",
    "                remote_tensors.append(data[-1].detach().requires_grad_())\n",
    "            else:\n",
    "                remote_tensors.append(\n",
    "                    data[-1].detach().move(models[i + 1].location).requires_grad_()\n",
    "                )\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        data.append(models[i](remote_tensors[-1]))\n",
    "\n",
    "        self.data = data\n",
    "        self.remote_tensors = remote_tensors\n",
    "\n",
    "        return data[-1]\n",
    "\n",
    "    def backward(self):\n",
    "        data = self.data\n",
    "        remote_tensors = self.remote_tensors\n",
    "\n",
    "        i = len(models) - 2\n",
    "        while i > -1:\n",
    "            if remote_tensors[i].location == data[i].location:\n",
    "                grads = remote_tensors[i].grad.copy()\n",
    "            else:\n",
    "                grads = remote_tensors[i].grad.copy().move(data[i].location)\n",
    "\n",
    "            data[i].backward(grads)\n",
    "            i -= 1\n",
    "\n",
    "    def zero_grads(self):\n",
    "        for opt in self.optimizers:\n",
    "            opt.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        for opt in self.optimizers:\n",
    "            opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/home/pavlito/miniconda3/envs/pyvertical-dev/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.15.3.so'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import syft as sy\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from src.dataloader import PartitionDistributingDataLoader\n",
    "from src.dataset import add_ids, partition_dataset\n",
    "\n",
    "hook = sy.TorchHook(torch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "data = add_ids(MNIST)(\".\", download=True, transform=ToTensor())  # add_ids adds unique IDs to data points\n",
    "\n",
    "# Split data\n",
    "data_partition1, data_partition2 = partition_dataset(data, remove_data=False, keep_order=True)\n",
    "\n",
    "# Batch data\n",
    "dataloader = PartitionDistributingDataLoader(data_partition1, data_partition2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Define our model segments\n",
    "\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 640]\n",
    "output_size = 10\n",
    "\n",
    "models = [\n",
    "    nn.Sequential(\n",
    "        nn.Linear(input_size, hidden_sizes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "        nn.ReLU(),\n",
    "    ),\n",
    "    nn.Sequential(nn.Linear(hidden_sizes[1], output_size), nn.LogSoftmax(dim=1)),\n",
    "]\n",
    "\n",
    "# Create optimisers for each segment and link to them\n",
    "optimizers = [\n",
    "    optim.SGD(model.parameters(), lr=0.03,)\n",
    "    for model in models\n",
    "]\n",
    "\n",
    "# create some workers\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "\n",
    "# Send Model Segments to model locations\n",
    "model_locations = [alice, bob]\n",
    "for model, location in zip(models, model_locations):\n",
    "    model.send(location)\n",
    "\n",
    "#Instantiate a SpliNN class with our distributed segments and their respective optimizers\n",
    "splitNN = SplitNN(models, optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, target, splitNN):\n",
    "    \n",
    "    #1) Zero our grads\n",
    "    splitNN.zero_grads()\n",
    "    \n",
    "    #2) Make a prediction\n",
    "    pred = splitNN.forward(x)\n",
    "    \n",
    "    #3) Figure out how much we missed by\n",
    "    criterion = nn.NLLLoss()\n",
    "    loss = criterion(pred, target)\n",
    "    \n",
    "    #4) Backprop the loss on the end layer\n",
    "    loss.backward()\n",
    "    \n",
    "    #5) Feed Gradients backward through the nework\n",
    "    splitNN.backward()\n",
    "    \n",
    "    #6) Change the weights\n",
    "    splitNN.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 1.1379796266555786\n",
      "Epoch 1 - Training loss: 0.38587328791618347\n",
      "Epoch 2 - Training loss: 0.3184337019920349\n",
      "Epoch 3 - Training loss: 0.2822173535823822\n",
      "Epoch 4 - Training loss: 0.2552659809589386\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    running_loss = 0        \n",
    "    for (data, ids1), (labels, ids2) in dataloader:\n",
    "        # Train a model\n",
    "        data = data.send(models[0].location)\n",
    "        data = data.view(data.shape[0], -1)\n",
    "        labels = labels.send(models[-1].location)\n",
    "        loss = train(data, labels, splitNN)\n",
    "        running_loss += loss.get()\n",
    "\n",
    "    else:\n",
    "#       print(\"Epoch {} - Training loss: {}\".format(i, running_loss/len(dataloader)))\n",
    "        print(\"Epoch {} - Training loss: {}\".format(i, running_loss/469)) # Hardcoded for now. For 128 batch size this number should be 469."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels pointing to:  (Wrapper)>[PointerTensor | me:72070217692 -> bob:32837817051]\n",
      "Images pointing to:  (Wrapper)>[PointerTensor | me:59275959780 -> alice:6607304323]\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels pointing to: \", labels)\n",
    "print(\"Images pointing to: \", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
